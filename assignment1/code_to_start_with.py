# -*- coding: utf-8 -*-
# %% [markdown]

"""
Homework:

The folder '~//data//homework' contains data of Titanic with various features and survivals.

Try to use what you have learnt today to predict whether the passenger shall survive or not.

Evaluate your model.
# -*- coding: utf-8 -*- 
# %% [markdown] 

""" 
Homework: 

The folder '~//data//homework' contains data of Titanic with various features and survivals. 

Try to use what you have learnt today to predict whether the passenger shall survive or not. 

Evaluate your model. 
""" 
# %% 
# load data 
import pandas as pd 
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# è®¾ç½®ä¸­æ–‡æ˜¾ç¤º
plt.rcParams["font.family"] = ["SimHei", "WenQuanYi Micro Hei", "Heiti TC"]
plt.rcParams['axes.unicode_minus'] = False  # è§£å†³è´Ÿå·æ˜¾ç¤ºé—®é¢˜

# åŠ è½½æ•°æ®
# æ³¨æ„ï¼šæ ¹æ®å®é™…æ•°æ®è·¯å¾„è°ƒæ•´
data = pd.read_csv('"C:\Users\30358\Desktop\aiSummerCamp2025-master\day1\assignment1\data\train.csv"')  
if data.empty:
    raise ValueError("æ•°æ®åŠ è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶è·¯å¾„æ˜¯å¦æ­£ç¡®")

df = data.copy() 
print("æ•°æ®é›†æ ·æœ¬:\n", df.sample(10)) 
# %% 
# delete some features that are not useful for prediction 
df.drop(columns=['PassengerId', 'Name', 'Ticket', 'Cabin'], inplace=True) 
print("æ•°æ®ä¿¡æ¯:\n")
df.info() 
# %% 
# check if there is any NaN in the dataset 
print('\nå¤„ç†å‰æ˜¯å¦å­˜åœ¨ç¼ºå¤±å€¼: {}'.format(df.isnull().values.any())) 
# å¯¹äºç¼ºå¤±å€¼ï¼Œé™¤äº†ç›´æ¥åˆ é™¤ï¼Œä¹Ÿå¯ä»¥è€ƒè™‘ä½¿ç”¨å‡å€¼/ä¸­ä½æ•°å¡«å……
# è¿™é‡Œä½¿ç”¨ç®€å•åˆ é™¤æ³•
original_rows = df.shape[0]
df.dropna(inplace=True)
remaining_rows = df.shape[0]
print(f'åˆ é™¤ç¼ºå¤±å€¼åå‰©ä½™æ•°æ®æ¯”ä¾‹: {remaining_rows/original_rows:.2%}')
print('å¤„ç†åæ˜¯å¦å­˜åœ¨ç¼ºå¤±å€¼: {}'.format(df.isnull().values.any())) 
# %% 
# convert categorical data into numerical data using one-hot encoding 
# For example, a feature like sex with categories ['male', 'female'] would be transformed into two new binary features, sex_male and sex_female, represented by 0 and 1. 
df = pd.get_dummies(df) 
print("\nç‹¬çƒ­ç¼–ç åçš„æ•°æ®é›†æ ·æœ¬:\n", df.sample(10)) 
# %% 
# separate the features and labels 
X = df.drop('Survived', axis=1)  # ç‰¹å¾é›†
y = df['Survived']               # æ ‡ç­¾é›†
print('\nç‰¹å¾é›†å½¢çŠ¶:', X.shape)
print('æ ‡ç­¾é›†å½¢çŠ¶:', y.shape) 
# %% 
# train-test split 
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42  # ä½¿ç”¨å›ºå®šçš„éšæœºç§å­ä»¥ç¡®ä¿ç»“æœå¯å¤ç°
)
print('\nè®­ç»ƒé›†å¤§å°:', X_train.shape)
print('æµ‹è¯•é›†å¤§å°:', X_test.shape) 
# %% 
# build model 
# build three classification models 
# SVM, KNN, Random Forest 

def train_and_evaluate_model(model, model_name, X_train, X_test, y_train, y_test):
    """è®­ç»ƒæ¨¡å‹å¹¶è¯„ä¼°æ€§èƒ½"""
    # è®­ç»ƒæ¨¡å‹
    model.fit(X_train, y_train)
    
    # é¢„æµ‹
    y_pred = model.predict(X_test)
    
    # è¯„ä¼°
    accuracy = accuracy_score(y_test, y_pred)
    print(f'\n{model_name} æ¨¡å‹å‡†ç¡®ç‡: {accuracy:.4f}')
    
    # æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(y_test, y_pred)
    print(f'æ··æ·†çŸ©é˜µ:\n{cm}')
    
    # åˆ†ç±»æŠ¥å‘Š
    print('åˆ†ç±»æŠ¥å‘Š:\n', classification_report(y_test, y_pred))
    
    # ç»˜åˆ¶æ··æ·†çŸ©é˜µ
    plt.figure(figsize=(8, 6))
    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)
    plt.title(f'{model_name} æ··æ·†çŸ©é˜µ')
    plt.colorbar()
    classes = ['æœªå­˜æ´»', 'å­˜æ´»']
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes)
    plt.yticks(tick_marks, classes)
    
    # åœ¨æ··æ·†çŸ©é˜µä¸Šæ ‡æ³¨æ•°å­—
    thresh = cm.max() / 2.
    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            plt.text(j, i, format(cm[i, j], 'd'),
                    horizontalalignment="center",
                    color="white" if cm[i, j] > thresh else "black")
    
    plt.ylabel('çœŸå®æ ‡ç­¾')
    plt.xlabel('é¢„æµ‹æ ‡ç­¾')
    plt.tight_layout()
    plt.show()
    
    return model, accuracy

# åˆå§‹åŒ–æ¨¡å‹
models = {
    'SVM': SVC(kernel='rbf', random_state=42),
    'KNN': KNeighborsClassifier(n_neighbors=5),  # é€šå¸¸ä»5å¼€å§‹å°è¯•
    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42)
}

# å­˜å‚¨å„æ¨¡å‹å‡†ç¡®ç‡ç”¨äºæ¯”è¾ƒ
model_accuracies = {}

# %% 
# predict and evaluate 
print("\n======= æ¨¡å‹è®­ç»ƒä¸è¯„ä¼° =======")
for name, model in models.items():
    trained_model, acc = train_and_evaluate_model(model, name, X_train, X_test, y_train, y_test)
    model_accuracies[name] = acc

# %% 
# æ¯”è¾ƒä¸åŒæ¨¡å‹æ€§èƒ½
print("\n======= æ¨¡å‹æ€§èƒ½æ¯”è¾ƒ =======")
best_model = max(model_accuracies, key=model_accuracies.get)
for name, acc in model_accuracies.items():
    print(f'{name}: {acc:.4f}' + (' ğŸ‘ˆ æœ€ä½³æ¨¡å‹' if name == best_model else ''))

# ç»˜åˆ¶æ¨¡å‹å‡†ç¡®ç‡æ¯”è¾ƒå›¾
plt.figure(figsize=(10, 6))
colors = ['lightblue' if name != best_model else 'green' for name in model_accuracies.keys()]
plt.bar(model_accuracies.keys(), model_accuracies.values(), color=colors)
plt.title('ä¸åŒæ¨¡å‹å‡†ç¡®ç‡æ¯”è¾ƒ')
plt.ylabel('å‡†ç¡®ç‡')
plt.ylim(0, 1.0)

# åœ¨æŸ±çŠ¶å›¾ä¸Šæ ‡æ³¨å‡†ç¡®ç‡
for i, v in enumerate(model_accuracies.values()):
    plt.text(i, v + 0.01, f'{v:.4f}', ha='center')

plt.tight_layout()
plt.show()

print("\nç»“è®º: åœ¨å½“å‰æ•°æ®é›†ä¸Šï¼Œ{}æ¨¡å‹è¡¨ç°æœ€ä½³ï¼Œå‡†ç¡®ç‡ä¸º{:.4f}".format(best_model, model_accuracies[best_model]))
